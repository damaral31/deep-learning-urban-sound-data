"""
M√≥dulo SoundLSTM - Rede Neural Recorrente (LSTM) para classifica√ß√£o de √°udio urbano

Este m√≥dulo implementa um modelo LSTM especializado em capturar depend√™ncias temporais
em espectrogramas de sons urbanos (tr√°fego, sirenes, buzinas, etc).

Diferen√ßa CNN vs LSTM:
- CNN: Excelente para detectar padr√µes ESPACIAIS (bordas, formas)
- LSTM: Excelente para capturar DEPEND√äNCIAS TEMPORAIS (sequ√™ncias, progress√£o)

Para √°udio urbano, LSTM √© melhor porque:
- Sons urbanos t√™m progress√£o temporal importante (in√≠cio, meio, fim de um som)
- Uma sirene evolui ao longo do tempo
- O contexto passado ajuda a prever o pr√≥ximo frame do √°udio
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class SoundLSTM(nn.Module):
    """
    Rede Neural Recorrente (LSTM) para classifica√ß√£o de sons urbanos.
    
    Arquitetura:
    1. Camada convolucional inicial: extrai features 2D do espectrograma
    2. Reshape em sequ√™ncia temporal: cada coluna do espectrograma = um timestep
    3. LSTM bidirecional: processa sequ√™ncia em ambas dire√ß√µes (forward + backward)
    4. Camadas fully-connected: classifica√ß√£o final
    
    Por que LSTM e n√£o RNN simples?
    - RNN simples: sofre de "vanishing gradient" (esquece informa√ß√µes antigas)
    - LSTM: tem "gates" que controlam o que lembrar e o que esquecer
    - Perfeito para sequ√™ncias longas como espectrogramas de √°udio
    
    Fluxo de dados:
    Input [batch, 1, 40, 174] (espectrograma)
         ‚Üì
    Conv [batch, 32, 40, 174] (features 2D)
         ‚Üì
    Reshape [batch, 174, 32*40] (sequ√™ncia temporal)
         ‚Üì
    LSTM Bidirecional [batch, 174, 256] (contexto temporal)
         ‚Üì
    FC layers [batch, 10] (classifica√ß√£o)
    """
    
    def __init__(self, num_classes=10, input_height=40, input_width=174,
                 hidden_size=128, num_layers=2, dropout_rate=0.5):
        """
        Inicializa o modelo LSTM.
        
        Args:
            num_classes (int): N√∫mero de categorias de √°udio (padr√£o: 10)
                              Classes t√≠picas Urban Sound: 
                              - 0: Air conditioner
                              - 1: Car horn
                              - 2: Children playing
                              - 3: Dog barking
                              - 4: Drilling
                              - 5: Engine idling
                              - 6: Gun shot
                              - 7: Jackhammer
                              - 8: Siren
                              - 9: Street music
            
            input_height (int): Altura do espectrograma (padr√£o: 40 - mel bins)
                               Tipicamente 40 para espectrogramas de mel
            
            input_width (int): Largura do espectrograma (padr√£o: 174 - frames)
                              Corresponde ao n√∫mero de frames temporais
            
            hidden_size (int): Dimens√£o do estado oculto LSTM (padr√£o: 128)
                              Quanto maior = mais capaz de aprender, mas mais lento
            
            num_layers (int): N√∫mero de camadas LSTM empilhadas (padr√£o: 2)
                             Mais camadas = mais profundo, mas risco de overfitting
            
            dropout_rate (float): Taxa de dropout para regulariza√ß√£o (padr√£o: 0.5)
        """
        super(SoundLSTM, self).__init__()
        
        # ========== CAMADA CONVOLUCIONAL INICIAL ==========
        # Prop√≥sito: Extrair features 2D do espectrograma
        # Uma convolu√ß√£o pode ser suficiente para pr√©-processar
        # (diferente de CNN que usa 3+ camadas)
        
        # Conv1: Primeira e √∫nica convolu√ß√£o
        # - in_channels=1: espectrograma √© monocanal
        # - out_channels=32: extrai 32 caracter√≠sticas
        # - kernel_size=3: filtro 3x3 para padr√µes locais
        # - padding=1: mant√©m o tamanho
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        
        # BatchNorm2d: normaliza outputs da convolu√ß√£o
        # Benef√≠cios:
        # - Treinamento mais est√°vel
        # - Permite maiores learning rates
        # - Reduz depend√™ncia de inicializa√ß√£o de pesos
        self.bn_conv1 = nn.BatchNorm2d(32)
        
        # ReLU: ativa√ß√£o n√£o-linear
        # J√° definida no forward, mas poderia ser aqui tamb√©m
        
        # MaxPool: reduz altura (largura mantida para sequ√™ncia temporal)
        # - kernel_size=(2, 1): reduz altura por 2, n√£o mexe na largura
        # - Por qu√™? Queremos manter a dimens√£o temporal (largura) para LSTM
        # Ap√≥s pool: 40‚Üí20 (altura), 174‚Üí174 (largura)
        self.pool_conv = nn.MaxPool2d(kernel_size=(2, 1))
        
        # ========== CAMADA LSTM BIDIRECIONAL ==========
        # Prop√≥sito: Capturar depend√™ncias temporais em ambas dire√ß√µes
        
        # C√°lculo do tamanho de entrada LSTM:
        # Ap√≥s convu√ß√£o + pooling: [batch, 32 canais, 20 altura, 174 largura]
        # Queremos tratar a largura como sequ√™ncia temporal
        # Cada timestep = 32 * 20 = 640 features
        lstm_input_size = 32 * (input_height // 2)  # 32 * 20 = 640
        
        # LSTM bidirecional
        # - input_size: 640 (features de entrada por timestep)
        # - hidden_size: 128 (tamanho estado oculto)
        # - num_layers: 2 (2 camadas LSTM empilhadas)
        # - bidirectional: True (processa em ambas dire√ß√µes)
        #   * Forward: 0‚Üí1‚Üí2‚Üí...‚Üí174
        #   * Backward: 174‚Üí...‚Üí2‚Üí1‚Üí0
        #   * Output concatenado: [forward, backward] = 256 features
        # - batch_first: True (input shape: [batch, seq, features])
        # - dropout: 0.5 (dropout entre camadas, reduz overfitting)
        self.lstm = nn.LSTM(
            input_size=lstm_input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=True,
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0
        )
        
        # ========== CAMADAS FULLY-CONNECTED ==========
        # Depois de processar sequ√™ncia temporal com LSTM,
        # usamos FC layers para classifica√ß√£o
        
        # Nota: hidden_size * 2 porque LSTM √© bidirecional
        # Forward + Backward = 128 + 128 = 256 features
        
        # fc1: Primeira camada densa com dropout
        # - input: hidden_size * 2 = 256 (output bidirecional LSTM)
        # - output: 128 (reduz dimensionalidade)
        self.fc1 = nn.Linear(hidden_size * 2, 128)
        
        # BatchNorm1d: normaliza outputs da fc1
        # Mesmo conceito que BatchNorm2d, mas para tensores 1D
        self.bn_fc1 = nn.BatchNorm1d(128)
        
        # fc2: Segunda camada densa (camada de sa√≠da)
        # - input: 128 (da fc1)
        # - output: num_classes = 10 (classifica√ß√£o final)
        self.fc2 = nn.Linear(128, num_classes)
        
        # ========== REGULARIZA√á√ÉO ==========
        # Dropout aplicado entre FC layers durante treinamento
        # Taxa: 0.5 = 50% dos neur√¥nios desativados aleatoriamente
        self.dropout = nn.Dropout(dropout_rate)
        
        # Guardar hiperpar√¢metros para refer√™ncia
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_classes = num_classes

    def forward(self, x):
        """
        Define o fluxo de dados atrav√©s da rede (forward pass).
        
        Este m√©todo √© chamado automaticamente quando fazer:
        output = model(input)
        
        Args:
            x (torch.Tensor): Input espectrograma com shape [batch_size, 1, altura, largura]
                            Exemplo: [8, 1, 40, 174]
                            8 espectrogramas de √°udio urbano
                            
        Returns:
            torch.Tensor: Logits de sa√≠da com shape [batch_size, num_classes]
                         Exemplo: [8, 10]
                         8 conjuntos de 10 scores (um por classe de som)
        """
        batch_size = x.size(0)
        
        # ========== PASSO 1: CONVOLU√á√ÉO + BATCH NORM + ReLU ==========
        # Prop√≥sito: Extrair features 2D do espectrograma
        #
        # Input shape: [batch, 1, 40, 174]
        # Esperado output: [batch, 32, 40, 174] (sem pool ainda)
        
        # Aplicar convolu√ß√£o
        x = self.conv1(x)  # [batch, 32, 40, 174]
        
        # Batch normalization: normaliza outputs
        x = self.bn_conv1(x)
        
        # ReLU: ativa√ß√£o n√£o-linear
        # max(0, x) ‚Üí apenas valores positivos
        x = F.relu(x)
        
        # ========== PASSO 2: MAX POOLING (ALTURA) ==========
        # Reduz altura por 2, mant√©m largura
        # [batch, 32, 40, 174] ‚Üí [batch, 32, 20, 174]
        x = self.pool_conv(x)
        
        # ========== PASSO 3: RESHAPE PARA SEQU√äNCIA TEMPORAL ==========
        # Converte tensor 4D em 3D para LSTM
        #
        # Antes: [batch, 32, 20, 174]  (4D: batch, canais, altura, largura)
        # Depois: [batch, 174, 640]     (3D: batch, timesteps, features)
        #
        # Interpreta√ß√£o:
        # - Dimension temporal (174 timesteps) = cada coluna do espectrograma
        # - Features (640) = 32 canais √ó 20 altura
        # - Cada timestep representa um frame temporal do √°udio
        
        # Dimens√µes:
        # x.size(0) = batch size
        # x.size(3) = 174 (largura = n√∫mero de frames)
        # -1 = calcular automaticamente (32 * 20 = 640)
        
        x = x.view(batch_size, x.size(3), -1)
        # Agora: [batch, 174, 640]
        
        # ========== PASSO 4: LSTM BIDIRECIONAL ==========
        # Prop√≥sito: Capturar contexto temporal em ambas dire√ß√µes
        #
        # LSTM processa sequ√™ncia e retorna:
        # - lstm_out: output de todos os timesteps
        # - (h_n, c_n): estado final (hidden e cell state)
        #
        # Processamento bidirecional:
        # Forward:  Frame0 ‚Üí Frame1 ‚Üí Frame2 ‚Üí ... ‚Üí Frame173
        # Backward: Frame173 ‚Üí ... ‚Üí Frame2 ‚Üí Frame1 ‚Üí Frame0
        # Result: Concatena√ß√£o de ambas = contexto completo
        
        lstm_out, (h_n, c_n) = self.lstm(x)
        # lstm_out shape: [batch, 174, 256]
        #                 (256 = 128 hidden_size * 2 directions)
        # h_n shape: [num_layers * 2, batch, 128]  (* 2 por bidirecional)
        # c_n shape: [num_layers * 2, batch, 128]
        
        # ========== PASSO 5: POOL TEMPORAL ==========
        # Strategy: Usar apenas o √∫ltimo output LSTM
        # Justificativa:
        # - Cont√©m contexto de toda a sequ√™ncia (LSTM propaga contexto)
        # - Mais est√°vel que m√©dia
        # - Alternativa: usar m√©dia de todos os timesteps ou attention
        #
        # Outro estrat√©gia seria:
        # x = lstm_out.mean(dim=1)  # M√©dia de todos timesteps
        #
        # Ou usar aten√ß√£o para aprender pesos dinamicamente
        
        last_output = lstm_out[:, -1, :]
        # Shape: [batch, 256]
        # -1 seleciona √∫ltimo timestep
        
        # ========== PASSO 6: PRIMEIRA CAMADA FULLY-CONNECTED ==========
        # Reduz dimensionalidade e aprende combina√ß√µes n√£o-lineares
        
        x = self.fc1(last_output)  # [batch, 128]
        
        # Batch normalization: estabiliza treinamento
        x = self.bn_fc1(x)
        
        # ReLU: ativa√ß√£o n√£o-linear
        x = F.relu(x)
        
        # Dropout: regulariza√ß√£o para evitar overfitting
        x = self.dropout(x)
        
        # ========== PASSO 7: SEGUNDA CAMADA FULLY-CONNECTED (SA√çDA) ==========
        # Camada final: produz scores para cada classe
        #
        # IMPORTANTE: SEM ativa√ß√£o aqui!
        # Retornamos logits (scores brutos)
        # CrossEntropyLoss vai aplicar softmax internamente
        
        x = self.fc2(x)  # [batch, 10]
        
        # ========== RESUMO DO FLUXO COMPLETO ==========
        # [batch, 1, 40, 174]        ‚Üê Espectrograma de √°udio urbano
        #   ‚Üì
        # Conv + BN + ReLU + Pool
        # [batch, 32, 20, 174]       ‚Üê Features 2D extra√≠das
        #   ‚Üì
        # Reshape para sequ√™ncia
        # [batch, 174, 640]          ‚Üê 174 timesteps com 640 features cada
        #   ‚Üì
        # LSTM Bidirecional
        # [batch, 256]               ‚Üê Contexto temporal (√∫ltimo timestep)
        #   ‚Üì
        # FC1 + BN + ReLU + Dropout
        # [batch, 128]               ‚Üê Features aprendidas
        #   ‚Üì
        # FC2
        # [batch, 10]                ‚Üê Scores finais (logits)
        #   ‚Üì
        # Softmax (durante inference)
        # [batch, 10]                ‚Üê Probabilidades (soma = 1)
        #   ‚Üì
        # Argmax
        # [batch]                    ‚Üê Classe final (0-9)
        
        return x


class SoundLSTMAttention(nn.Module):
    """
    Vers√£o melhorada de SoundLSTM com Attention Mechanism.
    
    Melhoria:
    - Ao inv√©s de usar apenas o √∫ltimo output LSTM,
    - Usa attention para aprender quais timesteps s√£o mais importantes
    - Melhor que pooling simples, especialmente para sounds curtos ou longos
    
    Uso: Mesma interface que SoundLSTM
    """
    
    def __init__(self, num_classes=10, input_height=40, input_width=174,
                 hidden_size=128, num_layers=2, dropout_rate=0.5):
        """Inicializa LSTM com Attention."""
        super(SoundLSTMAttention, self).__init__()
        
        # Mesmas camadas da vers√£o anterior
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.bn_conv1 = nn.BatchNorm2d(32)
        self.pool_conv = nn.MaxPool2d(kernel_size=(2, 1))
        
        lstm_input_size = 32 * (input_height // 2)
        
        self.lstm = nn.LSTM(
            input_size=lstm_input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=True,
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0
        )
        
        # ========== ATTENTION MECHANISM ==========
        # Aprende quais timesteps s√£o mais relevantes
        
        # Linear layer para calcular scores de aten√ß√£o
        self.attention = nn.Linear(hidden_size * 2, 1)
        
        # FC layers
        self.fc1 = nn.Linear(hidden_size * 2, 128)
        self.bn_fc1 = nn.BatchNorm1d(128)
        self.fc2 = nn.Linear(128, num_classes)
        
        self.dropout = nn.Dropout(dropout_rate)
        self.hidden_size = hidden_size
    
    def forward(self, x):
        """Forward pass com attention."""
        batch_size = x.size(0)
        
        # Convolu√ß√£o
        x = self.conv1(x)
        x = self.bn_conv1(x)
        x = F.relu(x)
        x = self.pool_conv(x)
        
        # Reshape
        x = x.view(batch_size, x.size(3), -1)
        
        # LSTM
        lstm_out, _ = self.lstm(x)
        # [batch, 174, 256]
        
        # ========== ATTENTION ==========
        # Calcular scores de aten√ß√£o para cada timestep
        attention_scores = self.attention(lstm_out)
        # [batch, 174, 1]
        
        # Aplicar softmax para normalizar scores
        attention_weights = F.softmax(attention_scores, dim=1)
        # [batch, 174, 1]
        
        # Ponderar outputs LSTM pelo peso de aten√ß√£o
        weighted_output = lstm_out * attention_weights
        # [batch, 174, 256] * [batch, 174, 1] = [batch, 174, 256]
        
        # Somar ponderado (reduz timesteps)
        context = weighted_output.sum(dim=1)
        # [batch, 256]
        
        # FC layers
        x = self.fc1(context)
        x = self.bn_fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x


# ============================================================================
# EXEMPLOS DE USO E TESTES
# ============================================================================
if __name__ == "__main__":
    print("=" * 70)
    print("MODELOS LSTM PARA CLASSIFICA√á√ÉO DE √ÅUDIO URBANO")
    print("=" * 70)
    
    # ===== CONFIGURA√á√ÉO =====
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nüì± Usando device: {device}")
    
    # Par√¢metros
    batch_size = 8
    num_classes = 10  # Urban Sound: 10 categorias
    input_height = 40  # Mel bins
    input_width = 174  # Frames temporais
    
    # Criar input simulado (batch de 8 espectrogramas)
    dummy_input = torch.randn(batch_size, 1, input_height, input_width).to(device)
    print(f"\nüìä Input shape: {dummy_input.shape}")
    print(f"   Interpreta√ß√£o: {batch_size} espectrogramas, "
          f"{input_height} mel bins, {input_width} frames")
    
    # ===== MODELO 1: LSTM B√ÅSICO =====
    print("\n" + "=" * 70)
    print("MODELO 1: SoundLSTM (LSTM Bidirecional)")
    print("=" * 70)
    
    lstm_model = SoundLSTM(
        num_classes=num_classes,
        input_height=input_height,
        input_width=input_width,
        hidden_size=128,
        num_layers=2,
        dropout_rate=0.5
    ).to(device)
    
    # Contar par√¢metros
    lstm_params = sum(p.numel() for p in lstm_model.parameters())
    print(f"\nüìà Total de par√¢metros: {lstm_params:,}")
    
    # Forward pass
    with torch.no_grad():
        lstm_output = lstm_model(dummy_input)
    
    print(f"‚úÖ Output shape: {lstm_output.shape}")
    print(f"   Interpreta√ß√£o: {batch_size} amostras, {num_classes} classes")
    
    # Calcular probabilidades (softmax)
    probs = torch.softmax(lstm_output, dim=1)
    predicted_classes = torch.argmax(probs, dim=1)
    print(f"\nüìã Exemplo de predi√ß√£o:")
    print(f"   Scores brutos: {lstm_output[0]}")
    print(f"   Probabilidades: {probs[0]}")
    print(f"   Classe predita: {predicted_classes[0].item()}")
    
    # ===== MODELO 2: LSTM COM ATTENTION =====
    print("\n" + "=" * 70)
    print("MODELO 2: SoundLSTMAttention (LSTM + Attention)")
    print("=" * 70)
    
    attention_model = SoundLSTMAttention(
        num_classes=num_classes,
        input_height=input_height,
        input_width=input_width,
        hidden_size=128,
        num_layers=2,
        dropout_rate=0.5
    ).to(device)
    
    # Contar par√¢metros
    attention_params = sum(p.numel() for p in attention_model.parameters())
    print(f"\nüìà Total de par√¢metros: {attention_params:,}")
    
    # Forward pass
    with torch.no_grad():
        attention_output = attention_model(dummy_input)
    
    print(f"‚úÖ Output shape: {attention_output.shape}")
    
    # ===== COMPARA√á√ÉO =====
    print("\n" + "=" * 70)
    print("COMPARA√á√ÉO")
    print("=" * 70)
    print(f"SoundLSTM:           {lstm_params:,} par√¢metros")
    print(f"SoundLSTMAttention:  {attention_params:,} par√¢metros")
    print(f"\nDiferen√ßa: {attention_params - lstm_params:,} par√¢metros")
    print(f"Raz√£o: {attention_params / lstm_params:.2f}x")
    
    # ===== INFORMA√á√ïES T√âCNICAS =====
    print("\n" + "=" * 70)
    print("INFORMA√á√ïES T√âCNICAS")
    print("=" * 70)
    print(f"""
Urban Sound Dataset (10 classes):
  0: Air conditioner      6: Gun shot
  1: Car horn             7: Jackhammer
  2: Children playing     8: Siren
  3: Dog barking          9: Street music
  4: Drilling
  5: Engine idling

Caracter√≠sticas LSTM para √°udio urbano:
  ‚úì Captura depend√™ncias temporais
  ‚úì Bidirecional: contexto anterior e posterior
  ‚úì Melhor que CNN para sequ√™ncias longas
  ‚úì Attention: aprende frames importantes

Tamanho t√≠pico espectrograma:
  - Altura: 40 (mel-frequency bins)
  - Largura: 174 (frames temporais)
  - Dura√ß√£o: ~4 segundos (44.1 kHz, hop=512)
    """)
    
    print("\n" + "=" * 70)
    print("‚úÖ Testes completados com sucesso!")
    print("=" * 70)
