{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a033d1a",
   "metadata": {},
   "source": [
    "# üéµ Treinamento de Modelos Deep Learning - Urban Sound Classification\n",
    "\n",
    "Este notebook implementa o treinamento completo de v√°rios modelos de deep learning para classifica√ß√£o de sons urbanos:\n",
    "- **CNN** (Convolutional Neural Network)\n",
    "- **RNN** (Recurrent Neural Network)\n",
    "- **GRU** (Gated Recurrent Unit)\n",
    "- **BiRNN** (Bidirectional RNN)\n",
    "- **LSTM** (Long Short-Term Memory)\n",
    "- **LSTM com Attention**\n",
    "\n",
    "## Dataset: Urban Sound 8K\n",
    "- 10 classes de sons urbanos\n",
    "- ~8732 samples de √°udio\n",
    "- 4 segundos cada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521872d1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Imports e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309218ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'soundata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     18\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataloader\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCNN\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SoundCNN\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRNN\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SoundRNN, SoundGRU, SoundBiRNN\n",
      "File \u001b[1;32mc:\\Users\\diogo\\OneDrive\\Documents\\GitHub\\deep-learning-urban-sound-data\\dataloader.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msoundata\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msoundata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Clip\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'soundata'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import librosa\n",
    "import librosa.display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports dos nossos m√≥dulos\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from dataloader import Dataloader\n",
    "from models.CNN import SoundCNN\n",
    "from models.RNN import SoundRNN, SoundGRU, SoundBiRNN\n",
    "from models.LSTM import SoundLSTM, SoundLSTMAttention\n",
    "from config import DEVICE, TRAINING, DATASET, MODELS_DIR, PLOTS_DIR, LOGS_DIR\n",
    "\n",
    "print(f\"‚úÖ Imports completados\")\n",
    "print(f\"üì± Device dispon√≠vel: {DEVICE}\")\n",
    "print(f\"üéØ Configura√ß√µes:\")\n",
    "print(f\"   - Epochs: {TRAINING['epochs']}\")\n",
    "print(f\"   - Batch size: {TRAINING['batch_size']}\")\n",
    "print(f\"   - Learning rate: {TRAINING['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf6302",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Fun√ß√£o de Preprocessamento de √Åudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b89a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(clip, target_length=174, n_mels=40, sr=22050):\n",
    "    \"\"\"\n",
    "    Preprocessa √°udio para espectrograma mel.\n",
    "    \n",
    "    Args:\n",
    "        clip: soundata.core.Clip object\n",
    "        target_length: n√∫mero de frames temporais desejado\n",
    "        n_mels: n√∫mero de mel bins\n",
    "        sr: sample rate\n",
    "    \n",
    "    Returns:\n",
    "        mel_spectrogram: numpy array [n_mels, target_length]\n",
    "    \"\"\"\n",
    "    # Carregar √°udio\n",
    "    audio_data, original_sr = clip.audio\n",
    "    \n",
    "    # Resample se necess√°rio\n",
    "    if original_sr != sr:\n",
    "        audio_data = librosa.resample(audio_data, orig_sr=original_sr, target_sr=sr)\n",
    "    \n",
    "    # Converter para mono se necess√°rio\n",
    "    if len(audio_data.shape) > 1:\n",
    "        audio_data = np.mean(audio_data, axis=0)\n",
    "    \n",
    "    # Calcular mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=sr,\n",
    "        n_mels=n_mels,\n",
    "        n_fft=2048,\n",
    "        hop_length=512\n",
    "    )\n",
    "    \n",
    "    # Converter para dB\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Ajustar comprimento (pad ou truncate)\n",
    "    if mel_spec_db.shape[1] < target_length:\n",
    "        pad_width = target_length - mel_spec_db.shape[1]\n",
    "        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mel_spec_db = mel_spec_db[:, :target_length]\n",
    "    \n",
    "    return mel_spec_db\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o de preprocessamento definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b0d9b",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Dataset PyTorch Personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset PyTorch para Urban Sound 8K.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, indices=None):\n",
    "        self.dataloader = dataloader\n",
    "        self.indices = indices if indices is not None else list(range(len(dataloader)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        clip, label = self.dataloader[actual_idx]\n",
    "        \n",
    "        # Preprocessar\n",
    "        mel_spec = preprocess_audio(clip)\n",
    "        \n",
    "        # Converter para tensor\n",
    "        mel_spec_tensor = torch.FloatTensor(mel_spec).unsqueeze(0)  # Add channel dimension\n",
    "        label_tensor = torch.LongTensor([label])[0]\n",
    "        \n",
    "        return mel_spec_tensor, label_tensor\n",
    "\n",
    "print(\"‚úÖ Dataset PyTorch definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6a723",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Carregar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path do dataset - ajuste conforme necess√°rio\n",
    "DATASET_PATH = r\"C:\\Users\\diogo\\OneDrive\\Documents\\UrbanSound8K\\UrbanSound8K\"\n",
    "\n",
    "print(\"üîÑ Carregando dataset...\")\n",
    "dataloader = Dataloader(DATASET_PATH, verbose=False)\n",
    "\n",
    "# Criar dataset PyTorch\n",
    "full_dataset = UrbanSoundDataset(dataloader)\n",
    "\n",
    "# Split train/validation/test (70/15/15)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset carregado:\")\n",
    "print(f\"   - Total: {total_size} samples\")\n",
    "print(f\"   - Train: {train_size} samples\")\n",
    "print(f\"   - Validation: {val_size} samples\")\n",
    "print(f\"   - Test: {test_size} samples\")\n",
    "\n",
    "# Criar DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAINING['batch_size'], shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=TRAINING['batch_size'], shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TRAINING['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"‚úÖ DataLoaders criados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579dec6",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualizar Exemplos do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar alguns espectrogramas\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "class_mapping = dataloader.get_label_mapping()\n",
    "\n",
    "for i in range(6):\n",
    "    mel_spec, label = train_dataset[i]\n",
    "    \n",
    "    axes[i].imshow(mel_spec.squeeze().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[i].set_title(f\"Class: {class_mapping[label.item()]}\")\n",
    "    axes[i].set_xlabel('Time Frames')\n",
    "    axes[i].set_ylabel('Mel Bins')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'dataset_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Exemplos visualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8719c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Fun√ß√µes de Treino e Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Treina o modelo por uma √©poca.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Valida o modelo.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc='Validating', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs, device, model_name, patience=10):\n",
    "    \"\"\"\n",
    "    Treina o modelo com early stopping.\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéØ Treinando modelo: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_no_improve = 0\n",
    "            print(f\"‚úÖ Novo melhor modelo! Val Acc: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"‚ö†Ô∏è Sem melhoria por {epochs_no_improve} √©pocas\")\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nüõë Early stopping ap√≥s {epoch+1} √©pocas\")\n",
    "            break\n",
    "    \n",
    "    # Restaurar melhor modelo\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n‚è±Ô∏è Tempo total de treino: {elapsed_time/60:.2f} minutos\")\n",
    "    print(f\"üèÜ Melhor Val Acc: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return history, best_val_acc\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes de treino definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158b58f",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Fun√ß√£o para Avaliar no Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, model_name, class_mapping):\n",
    "    \"\"\"\n",
    "    Avalia o modelo no test set e gera m√©tricas e gr√°ficos.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"\\nüîç Avaliando {model_name} no test set...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\nüìä M√©tricas do Test Set:\")\n",
    "    print(f\"   - Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"   - F1-Score (weighted): {test_f1:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nüìã Classification Report:\")\n",
    "    target_names = [class_mapping[i] for i in range(len(class_mapping))]\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}\\nAccuracy: {test_acc*100:.2f}%')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / f'confusion_matrix_{model_name}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return test_acc, test_f1, cm\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o de avalia√ß√£o definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7e6a6",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Fun√ß√£o para Plotar Hist√≥rico de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b88e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Plota loss e accuracy ao longo das √©pocas.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{model_name} - Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "    ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title(f'{model_name} - Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / f'training_history_{model_name}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o de plot definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1c6ce",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Treinar Todos os Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91386ca",
   "metadata": {},
   "source": [
    "### 9.1 Configurar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir todos os modelos\n",
    "models_config = {\n",
    "    'SoundCNN': SoundCNN(num_classes=10),\n",
    "    'SoundRNN': SoundRNN(num_classes=10, input_height=40, input_width=174),\n",
    "    'SoundGRU': SoundGRU(num_classes=10, input_height=40, input_width=174),\n",
    "    'SoundBiRNN': SoundBiRNN(num_classes=10, input_height=40, input_width=174),\n",
    "    'SoundLSTM': SoundLSTM(num_classes=10, input_height=40, input_width=174),\n",
    "    'SoundLSTMAttention': SoundLSTMAttention(num_classes=10, input_height=40, input_width=174)\n",
    "}\n",
    "\n",
    "# Exibir n√∫mero de par√¢metros\n",
    "print(\"\\nüìä Modelos configurados:\")\n",
    "print(\"=\"*60)\n",
    "for name, model in models_config.items():\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:20s} - {num_params:>12,} par√¢metros\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae58e8",
   "metadata": {},
   "source": [
    "### 9.2 Loop de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenar resultados\n",
    "results = {}\n",
    "\n",
    "# Loop atrav√©s de cada modelo\n",
    "for model_name, model in models_config.items():\n",
    "    print(f\"\\n\\n{'#'*80}\")\n",
    "    print(f\"# TREINANDO: {model_name}\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "    \n",
    "    # Mover modelo para device\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Definir loss e optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=TRAINING['learning_rate'],\n",
    "        weight_decay=TRAINING['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo\n",
    "    history, best_val_acc = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=TRAINING['epochs'],\n",
    "        device=DEVICE,\n",
    "        model_name=model_name,\n",
    "        patience=TRAINING['early_stopping_patience']\n",
    "    )\n",
    "    \n",
    "    # Plotar hist√≥rico de treino\n",
    "    plot_training_history(history, model_name)\n",
    "    \n",
    "    # Avaliar no test set\n",
    "    test_acc, test_f1, cm = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        device=DEVICE,\n",
    "        model_name=model_name,\n",
    "        class_mapping=dataloader.get_label_mapping()\n",
    "    )\n",
    "    \n",
    "    # Salvar modelo\n",
    "    model_path = MODELS_DIR / f\"{model_name}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"\\nüíæ Modelo salvo em: {model_path}\")\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results[model_name] = {\n",
    "        'history': history,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ {model_name} conclu√≠do!\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\n\\nüéâ TODOS OS MODELOS TREINADOS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932184c4",
   "metadata": {},
   "source": [
    "## üîü Compara√ß√£o Final dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela de compara√ß√£o\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    num_params = sum(p.numel() for p in models_config[name].parameters())\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Parameters': f\"{num_params:,}\",\n",
    "        'Best Val Acc (%)': f\"{result['best_val_acc']:.2f}\",\n",
    "        'Test Acc (%)': f\"{result['test_acc']*100:.2f}\",\n",
    "        'Test F1-Score': f\"{result['test_f1']:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test Acc (%)', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARA√á√ÉO FINAL DOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Salvar tabela\n",
    "comparison_df.to_csv(LOGS_DIR / 'model_comparison.csv', index=False)\n",
    "print(f\"\\nüíæ Tabela salva em: {LOGS_DIR / 'model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4aa60",
   "metadata": {},
   "source": [
    "### Gr√°fico de Barras - Compara√ß√£o de Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df202b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para plot\n",
    "model_names = [name for name in results.keys()]\n",
    "test_accs = [result['test_acc']*100 for result in results.values()]\n",
    "val_accs = [result['best_val_acc'] for result in results.values()]\n",
    "\n",
    "# Criar gr√°fico\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, val_accs, width, label='Best Validation Accuracy', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, test_accs, width, label='Test Accuracy', color='coral')\n",
    "\n",
    "# Adicionar labels\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Comparison - Validation vs Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "def add_values_on_bars(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "add_values_on_bars(bars1)\n",
    "add_values_on_bars(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'model_comparison_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Gr√°fico de compara√ß√£o salvo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa34a1",
   "metadata": {},
   "source": [
    "### Gr√°fico - Compara√ß√£o de Par√¢metros vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados\n",
    "params_list = [sum(p.numel() for p in models_config[name].parameters()) for name in model_names]\n",
    "\n",
    "# Criar scatter plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "for i, name in enumerate(model_names):\n",
    "    ax.scatter(params_list[i], test_accs[i], s=200, c=[colors[i]], \n",
    "               alpha=0.7, edgecolors='black', linewidth=2, label=name)\n",
    "\n",
    "ax.set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Complexity vs Performance', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Formatar eixo x\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'model_complexity_vs_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Gr√°fico de complexidade salvo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d32184",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ An√°lise por Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para o melhor modelo, analisar performance por classe\n",
    "best_model_name = max(results, key=lambda x: results[x]['test_acc'])\n",
    "best_model = models_config[best_model_name].to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(MODELS_DIR / f\"{best_model_name}.pt\"))\n",
    "\n",
    "print(f\"\\nüèÜ Melhor modelo: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {results[best_model_name]['test_acc']*100:.2f}%\")\n",
    "\n",
    "# Obter predi√ß√µes por classe\n",
    "best_model.eval()\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = best_model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i].item()\n",
    "            class_total[label] += 1\n",
    "            if predicted[i] == label:\n",
    "                class_correct[label] += 1\n",
    "\n",
    "# Calcular accuracy por classe\n",
    "class_mapping = dataloader.get_label_mapping()\n",
    "class_accuracies = []\n",
    "\n",
    "print(\"\\nüìä Accuracy por Classe:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        acc = 100 * class_correct[i] / class_total[i]\n",
    "        class_accuracies.append(acc)\n",
    "        print(f\"{class_mapping[i]:20s} - {acc:6.2f}% ({class_correct[i]}/{class_total[i]})\")\n",
    "    else:\n",
    "        class_accuracies.append(0)\n",
    "        print(f\"{class_mapping[i]:20s} - N/A (no samples)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plotar accuracy por classe\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "class_names = [class_mapping[i] for i in range(10)]\n",
    "bars = ax.barh(class_names, class_accuracies, color='teal', alpha=0.7)\n",
    "\n",
    "# Colorir barras por performance\n",
    "for i, bar in enumerate(bars):\n",
    "    if class_accuracies[i] >= 80:\n",
    "        bar.set_color('green')\n",
    "    elif class_accuracies[i] >= 60:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Per-Class Accuracy - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0, 100])\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Adicionar valores\n",
    "for i, v in enumerate(class_accuracies):\n",
    "    ax.text(v + 1, i, f'{v:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / f'per_class_accuracy_{best_model_name}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ An√°lise por classe conclu√≠da\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab903685",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e94617",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# üéâ TREINAMENTO COMPLETO!\")\n",
    "print(\"#\"*80)\n",
    "print(\"\\nüìÅ Arquivos salvos:\")\n",
    "print(f\"   - Modelos treinados: {MODELS_DIR}\")\n",
    "print(f\"   - Gr√°ficos: {PLOTS_DIR}\")\n",
    "print(f\"   - Logs: {LOGS_DIR}\")\n",
    "\n",
    "print(\"\\nüèÜ Ranking dos Modelos (por Test Accuracy):\")\n",
    "print(\"=\"*80)\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['test_acc'], reverse=True)\n",
    "for i, (name, result) in enumerate(sorted_results, 1):\n",
    "    print(f\"{i}. {name:20s} - {result['test_acc']*100:.2f}% (F1: {result['test_f1']:.4f})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚ú® Experimento conclu√≠do com sucesso!\")\n",
    "print(\"\\nPr√≥ximos passos sugeridos:\")\n",
    "print(\"   1. Analisar confusion matrices para identificar classes problem√°ticas\")\n",
    "print(\"   2. Experimentar com data augmentation\")\n",
    "print(\"   3. Ajustar hiperpar√¢metros do melhor modelo\")\n",
    "print(\"   4. Considerar ensemble de modelos\")\n",
    "print(\"   5. Testar com novos dados de √°udio\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diogo-ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
